{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lTVYho1DjXxlDX304Kd6xIDXjlq4s_lu",
      "authorship_tag": "ABX9TyMQwcTaroyV3IIg9GmQvU1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/FASTQ_Data_Miner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lp_oMPfTkAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import XML file with FASTQ files' links."
      ],
      "metadata": {
        "id": "KyX1AzRaHtAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile\n",
        "\n",
        "import rarfile\n",
        "import os\n",
        "\n",
        "dataset_drive_path = \"/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/SOURCES/ena_sra-experiment_20231030-2024.rar\"\n",
        "xml_path = \"/content/\"\n",
        "\n",
        "os.makedirs(xml_path, exist_ok=True)\n",
        "\n",
        "with rarfile.RarFile(dataset_drive_path) as rf:\n",
        "  rf.extractall(xml_path)"
      ],
      "metadata": {
        "id": "JRNTZp-zFgAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfec3ed-5d11-4163-b0e6-a6b212930741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a txt file with the report links."
      ],
      "metadata": {
        "id": "dhnm-c1pH24p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "# Create an XMLParser with error recovery\n",
        "parser = etree.XMLParser(recover=True)\n",
        "\n",
        "xml_path = \"/content/ena_sra-experiment_20231030-2024.xml\"\n",
        "\n",
        "# Try to parse the XML file\n",
        "try:\n",
        "    tree = etree.parse(xml_path, parser=parser)\n",
        "    root = tree.getroot()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"An error occurred while parsing the XML file:\")\n",
        "    print(e)\n",
        "    root = None\n",
        "\n",
        "# Download FASTQ files\n",
        "fastq_links = []\n",
        "\n",
        "if root is not None:\n",
        "    for experiment in root.findall('.//EXPERIMENT'):\n",
        "        experiment_links = experiment.findall('.//EXPERIMENT_LINKS/EXPERIMENT_LINK/XREF_LINK')\n",
        "        for link in experiment_links:\n",
        "            db = link.find('DB')\n",
        "            if db is not None and db.text == 'ENA-FASTQ-FILES':\n",
        "                fastq_link = link.find('ID')\n",
        "                if fastq_link is not None:\n",
        "                    fastq_links.append(fastq_link.text)\n",
        "\n",
        "'''\n",
        "# Print the list of FASTQ download links\n",
        "for link in fastq_links:\n",
        "    print(link)\n",
        "\n",
        "print(len(fastq_links))\n",
        "\n",
        "'''\n",
        "\n",
        "fastq_links_file = \"/content/fastq_links.txt\"\n",
        "with open(fastq_links_file, 'w') as file:\n",
        "    file.write('\\n'.join(fastq_links))"
      ],
      "metadata": {
        "id": "onRqLGKyimdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the file reports."
      ],
      "metadata": {
        "id": "RuD4gOfxZXmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "\n",
        "def download_reports(fastq_links, download_dir, num_files=None, seed=None):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        random.shuffle(fastq_links)\n",
        "\n",
        "    if num_files is not None:\n",
        "        fastq_links = fastq_links[:num_files]\n",
        "\n",
        "    for link in fastq_links:\n",
        "        file_name = link.split('/')[-1]\n",
        "        file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "        response = requests.get(link, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    f.write(chunk)\n",
        "\n",
        "    print(f\"Downloaded {len(fastq_links)} FASTQ files to {download_dir}.\")"
      ],
      "metadata": {
        "id": "AQzbCZ0FK-Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "def clear_folder(folder_path):\n",
        "    files_in_folder = len(os.listdir(folder_path))\n",
        "    print(f\"{files_in_folder} files found.\")\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path,file)\n",
        "        os.remove(file_path)\n",
        "\n",
        "    print(f\"{files_in_folder} removed from {folder_path}.\")"
      ],
      "metadata": {
        "id": "DVU0Wv0cQ3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clear_folder('/content/report_files')"
      ],
      "metadata": {
        "id": "luAprXAobk_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "DOWNLOAD_DIR = \"/content/report_files\"\n",
        "NUM_OF_FILES_TO_DOWNLOAD = 50\n",
        "SEED = 2 # 12,10,6,8,14,42,38 (Notes for the previous downloads)\n",
        "\n",
        "# Call the function\n",
        "download_reports(fastq_links, DOWNLOAD_DIR, num_files=NUM_OF_FILES_TO_DOWNLOAD, seed=SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yqYAwWfSJz",
        "outputId": "4a818203-57d2-4ead-91bf-c6e36875f904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 50 FASTQ files to /content/report_files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download FASTQ files from FTP links."
      ],
      "metadata": {
        "id": "VuLhpYq6g6tQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files are compressed (.fastq.gz) and a maximum file size is defined before downloading."
      ],
      "metadata": {
        "id": "rNmxEBJyzEX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum file size to download (MB)\n",
        "MAX_FILE_SIZE = 50\n",
        "\n",
        "# Specify the directory path to download files\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# Check file size before downloading and download a file from the ftp link\n",
        "def check_and_download_file(ftp_link):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= MAX_FILE_SIZE:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")"
      ],
      "metadata": {
        "id": "6UMfiHRplzC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/fastq_files"
      ],
      "metadata": {
        "id": "8WwRTDwP77iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "#clear_folder('/content/fastq_files')"
      ],
      "metadata": {
        "id": "bGrKKKafaTTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path containing the FTP links\n",
        "directory_path = \"/content/report_files/\"\n",
        "\n",
        "# Define the directory path in which the files will be downloaded\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# List all files in the directory\n",
        "all_files = os.listdir(directory_path)\n",
        "\n",
        "# Filter files that contain the necessary information\n",
        "info_files = [file for file in all_files if \"filereport\" in file]\n",
        "\n",
        "# Iterate through each info file\n",
        "for info_file in info_files:\n",
        "    file_path = os.path.join(directory_path, info_file)\n",
        "\n",
        "    # Read the content of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Take only the data line\n",
        "        lines = f.readlines()\n",
        "        data_line = lines[1]\n",
        "\n",
        "        #Check if there is data in the file and take only the first ftp link\n",
        "        if data_line is not None:\n",
        "            columns = data_line.strip().split('\\t')\n",
        "\n",
        "            if len(columns) >= 2:\n",
        "                ftp_link = columns[1]\n",
        "                ftp_link_1 = ftp_link.split(\";\")[0]\n",
        "                #print (ftp_link_1)\n",
        "\n",
        "                check_and_download_file(ftp_link_1) # Use ftp_link instead to download all files\n",
        "\n",
        "print(\"Downloaded FASTQ files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZZvb97DzkE0",
        "outputId": "5a476103-79c0-4e95-812a-370fefdee6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: SRR14167530_1.fastq.gz\n",
            "Downloaded: SRR16316187.fastq.gz\n",
            "Downloaded: SRR13841556_1.fastq.gz\n",
            "Downloaded: SRR14187989_1.fastq.gz\n",
            "Downloaded: SRR7781714_1.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR128/069/SRR12824069/SRR12824069.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR14592499_1.fastq.gz\n",
            "Downloaded: SRR7151338.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/ERR801/009/ERR8017479/ERR8017479_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR4446771.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR103/046/SRR10314346/SRR10314346.fastq.gz (Size exceeds maximum file size)\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR317/007/SRR3170977/SRR3170977_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR14592962_1.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/ERR809/001/ERR8093081/ERR8093081_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR7151162.fastq.gz\n",
            "Downloaded: SRR7994431_1.fastq.gz\n",
            "Downloaded: SRR14178498_1.fastq.gz\n",
            "Downloaded: SRR22969436_1.fastq.gz\n",
            "Downloaded: SRR14218203_1.fastq.gz\n",
            "Downloaded: SRR7761378.fastq.gz\n",
            "Downloaded: SRR8035074_1.fastq.gz\n",
            "Downloaded: SRR8048973_1.fastq.gz\n",
            "Downloaded: DRR350589_1.fastq.gz\n",
            "Downloaded: SRR13418604.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR136/056/SRR13681156/SRR13681156_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR14184457_1.fastq.gz\n",
            "Downloaded: SRR7998738_1.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR369/001/SRR3691991/SRR3691991_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR8034466_1.fastq.gz\n",
            "Downloaded: SRR21747441.fastq.gz\n",
            "Downloaded: SRR14172887_1.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR174/031/SRR17458331/SRR17458331.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR7980331_1.fastq.gz\n",
            "Downloaded: SRR13374461.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR778/006/SRR7781706/SRR7781706_1.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR13890938.fastq.gz\n",
            "Downloaded: SRR8001671_1.fastq.gz\n",
            "Downloaded: SRR2512817_subreads.fastq.gz\n",
            "Downloaded: SRR14591261_1.fastq.gz\n",
            "Downloaded: SRR7804410_1.fastq.gz\n",
            "Downloaded: SRR13848714_1.fastq.gz\n",
            "Downloaded: SRR8055960_1.fastq.gz\n",
            "Downloaded: SRR12994332_1.fastq.gz\n",
            "Skipping file: https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR225/062/SRR22535362/SRR22535362.fastq.gz (Size exceeds maximum file size)\n",
            "Downloaded: SRR8001063_1.fastq.gz\n",
            "Downloaded FASTQ files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decompress .FASTQ.GZ files."
      ],
      "metadata": {
        "id": "m72eyY3K81dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def decompress_gz_files(directory_path):\n",
        "    # List all files in the directory\n",
        "    all_files = os.listdir(directory_path)\n",
        "\n",
        "    # Filter files to keep only .gz files\n",
        "    gz_files = [file for file in all_files if file.endswith(\".gz\")]\n",
        "\n",
        "    # Decompress each .gz file\n",
        "    for gz_file in gz_files:\n",
        "        gz_file_path = os.path.join(directory_path, gz_file)\n",
        "\n",
        "        # Determine the output file name by removing the '.gz' extension\n",
        "        output_file_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "        # Decompress the .gz file\n",
        "        try:\n",
        "            with gzip.open(gz_file_path, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "            # Remove the original .gz file after decompression\n",
        "            os.remove(gz_file_path)\n",
        "\n",
        "        except (OSError, gzip.BadGzipFile) as e:\n",
        "            print(f\"Error decompressing {gz_file}: {e}\")\n",
        "\n",
        "    print(\"Decompression complete.\")"
      ],
      "metadata": {
        "id": "4UOmjB_M8yT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory path containing .gz files\n",
        "directory_path = \"/content/fastq_files/\"\n",
        "\n",
        "# Call the function to decompress .gz files in the specified directory\n",
        "decompress_gz_files(directory_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWI-SunrryG",
        "outputId": "b7de90da-6bf4-4501-a413-f4385d0a77b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompression complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to Drive."
      ],
      "metadata": {
        "id": "IJIJhV-n_cL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Define the source folder and its contents\n",
        "source_folder = '/content/fastq_files'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(source_folder, 'zip', source_folder)\n",
        "\n",
        "# Move the zip file to Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "seed = str(SEED)\n",
        "new_filename = f'fastq_files_seed_{seed}_{timestamp}.zip'\n",
        "\n",
        "shutil.move(source_folder + '.zip', f'/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/{new_filename}')\n",
        "\n",
        "print(f\"Folder '{source_folder}' has been zipped and uploaded to Google Drive with seed {seed}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_BJ719-bex",
        "outputId": "0639d4c0-462e-418e-a438-0c2d7f3aa891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/fastq_files' has been zipped and uploaded to Google Drive with seed 2.\n"
          ]
        }
      ]
    }
  ]
}