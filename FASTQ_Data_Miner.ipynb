{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/FASTQ_Data_Miner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gVeLJprX212"
      },
      "source": [
        "### Connect Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_oMPfTkAIs",
        "outputId": "abeb2294-bf91-4df0-a59d-462f395f853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53BLyNScX8AM"
      },
      "source": [
        "### Import Libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PCWR7WPX-4t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "!pip install rarfile\n",
        "import rarfile\n",
        "import zipfile\n",
        "import gzip\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "!pip install Bio\n",
        "from Bio import SeqIO\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX1AzRaHtAm"
      },
      "source": [
        "### Import XML file with report file links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqdS032j2YwV"
      },
      "source": [
        "The XML file contains links for one or more report files. These report files contain the FASTQ files to be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRNTZp-zFgAC"
      },
      "outputs": [],
      "source": [
        "def extract_files(source_path, destination_path):\n",
        "    try:\n",
        "        # Create the destination directory if it doesn't exist\n",
        "        os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "        # Check the file extension and extract or copy, accordingly\n",
        "        if source_path.endswith('.rar'):\n",
        "            with rarfile.RarFile(source_path) as rf:\n",
        "                rf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.zip'):\n",
        "            with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "                zf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.xml'):\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type. Only RAR, ZIP, and XML files are supported.\")\n",
        "\n",
        "        print(f\"Extraction completed successfully to: {destination_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2616fHGJi-",
        "outputId": "06f582c8-595e-4c83-88aa-d7b37e653118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully to: /content/\n"
          ]
        }
      ],
      "source": [
        "# Define the source and destination paths\n",
        "dataset_path = \"/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/SOURCES/PRJEB44548.xml\"\n",
        "xml_path = \"/content/\"\n",
        "\n",
        "# Extract the files\n",
        "extract_files(dataset_path, xml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnm-c1pH24p"
      },
      "source": [
        "### Create a CSV file with the report file links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ah-Xw3zARN9I"
      },
      "outputs": [],
      "source": [
        "def extract_report_file_links(text):\n",
        "    report_files = []\n",
        "    lines = text.split('\\n')\n",
        "    index = 0\n",
        "\n",
        "    while index < len(lines):\n",
        "        line = lines[index]\n",
        "\n",
        "        if \"<DB>ENA-FASTQ-FILES</DB>\" in line:\n",
        "            # Search for the link below this line\n",
        "            next_line = lines[index + 1]\n",
        "\n",
        "            if \"<ID>\" in next_line:\n",
        "                link_start_index = next_line.find(\"<ID><![CDATA[\")\n",
        "\n",
        "                if link_start_index != -1:\n",
        "                    link_end_index = next_line.find(\"]]></ID>\")\n",
        "\n",
        "                    if link_end_index != -1:\n",
        "                        fastq_link = next_line[link_start_index+len(\"<ID><![CDATA[\"):link_end_index]\n",
        "                        report_files.append(fastq_link)\n",
        "\n",
        "            # Move to the next line after the link\n",
        "            index += 2\n",
        "        else:\n",
        "            index += 1\n",
        "\n",
        "    return report_files\n",
        "\n",
        "def read_xml_file(xml_path):\n",
        "    try:\n",
        "        with open(xml_path, 'r') as f:\n",
        "            xml_text = f.read()\n",
        "\n",
        "        return xml_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading XML file: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "def save_links_to_csv(links, xml_path):\n",
        "    # Extract CSV file name from XML file name\n",
        "    base_name = os.path.basename(xml_path)\n",
        "    csv_filename = os.path.splitext(base_name)[0] + \"_links.csv\"\n",
        "    csv_path = os.path.join(\"/content/\", csv_filename)\n",
        "\n",
        "    try:\n",
        "        with open(csv_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\"Report file Links\"])\n",
        "\n",
        "            for link in links:\n",
        "                writer.writerow([link])\n",
        "\n",
        "        print(f\"Report file links saved to '{csv_path}' successfully.\")\n",
        "\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSjywkB4YMMC",
        "outputId": "10b0b2ff-1d32-457a-db93-e37c047227c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report file links saved to '/content/PRJEB44548_links.csv' successfully.\n"
          ]
        }
      ],
      "source": [
        "# Read the XML file\n",
        "xml_path += os.path.basename(dataset_path)\n",
        "xml_text = read_xml_file(xml_path)\n",
        "\n",
        "if xml_text:\n",
        "    report_files = extract_report_file_links(xml_text)\n",
        "\n",
        "    if report_files:\n",
        "        report_csv_path = save_links_to_csv(report_files, xml_path)\n",
        "\n",
        "    else:\n",
        "        print(\"No report file links found in the XML file.\")\n",
        "else:\n",
        "    print(\"Error reading XML file. Check the file path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUAhfeqaw3wq"
      },
      "source": [
        "Display the csv file information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CjxSkFe2v9M3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2e74d4ef-a31e-495f-ef3c-e42f2f9c6cd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef display_csv_content(csv_path):\\n    try:\\n        # Create a Pandas DataFrame\\n        df = pd.read_csv(csv_path)\\n\\n        # Display the DataFrame head and info\\n        print(df.head())\\n        print(df.info())\\n\\n    except Exception as e:\\n        print(f\"Error reading CSV file: {e}\")\\n\\ndisplay_csv_content(csv_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''\n",
        "def display_csv_content(csv_path):\n",
        "    try:\n",
        "        # Create a Pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Display the DataFrame head and info\n",
        "        print(df.head())\n",
        "        print(df.info())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "\n",
        "display_csv_content(csv_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuD4gOfxZXmS"
      },
      "source": [
        "### Download the report files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7h6cH99ye-M"
      },
      "source": [
        "Extract the desired number of file reports from the CSV file that contains the links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fcGcqEflgjNK"
      },
      "outputs": [],
      "source": [
        "def download_reports(report_links, download_dir, num_files=None, seed=None):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        random.shuffle(report_links)\n",
        "\n",
        "    if num_files is not None:\n",
        "        report_links = report_links[:num_files]\n",
        "\n",
        "    for link in report_links:\n",
        "        file_name = link.split('/')[-1]\n",
        "        file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "        response = requests.get(link, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    f.write(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-dbLngPmDXW"
      },
      "source": [
        "Define the hyperparameters and call the function to download the reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0MkeFWaEgl1B"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(report_csv_path)\n",
        "REPORT_LINKS = df['Report file Links']\n",
        "DOWNLOAD_DIR = \"/content/REPORT_FILES/\"\n",
        "NUM_FILES = 50\n",
        "SEED = 2\n",
        "\n",
        "download_reports(REPORT_LINKS, DOWNLOAD_DIR, NUM_FILES, SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMGlTzYlmNyf"
      },
      "source": [
        "### Extract the FTP links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SweHk9-egvz",
        "outputId": "732ed162-68e5-423d-bb16-3ff9055da53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46 FTPs saved.\n"
          ]
        }
      ],
      "source": [
        "for report_file in os.listdir(DOWNLOAD_DIR):\n",
        "    report_file_path = os.path.join(DOWNLOAD_DIR, report_file)\n",
        "\n",
        "    with open(report_file_path, 'r') as file:\n",
        "        # Skip the first line (header)\n",
        "        next(file)\n",
        "\n",
        "        # Initialize a list to store FTP links\n",
        "        ftp_links = []\n",
        "\n",
        "        # Iterate through each line in the file\n",
        "        for line in file:\n",
        "            # Split the line based on tab ('\\t') delimiter\n",
        "            line_parts = line.strip().split('\\t')\n",
        "\n",
        "            # Get the FTP links from the second column\n",
        "            ftp_links.extend(line_parts[1].split(';'))\n",
        "\n",
        "# Print the number of FTP links\n",
        "print(f\"{len(ftp_links)} FTPs saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download FASTQ.GZ files from FTP links."
      ],
      "metadata": {
        "id": "oBF1drz3D0dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6i0IvTWqkdv9"
      },
      "outputs": [],
      "source": [
        "def check_and_download_file(ftp_link, max_file_size, download_dir):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= max_file_size:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndmFsR4tFIe",
        "outputId": "90c42e73-b18e-4eb5-bee8-855236570835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: ERR5885024_1.fastq.gz\n",
            "Downloaded: ERR5885024_2.fastq.gz\n",
            "Downloaded: ERR5960498_1.fastq.gz\n",
            "Downloaded: ERR5960498_2.fastq.gz\n",
            "Downloaded: ERR5960500_1.fastq.gz\n",
            "Downloaded: ERR5960500_2.fastq.gz\n",
            "Downloaded: ERR6053338_1.fastq.gz\n",
            "Downloaded: ERR6053338_2.fastq.gz\n",
            "Downloaded: ERR5885023_1.fastq.gz\n",
            "Downloaded: ERR5885023_2.fastq.gz\n",
            "Downloaded: ERR5885025_1.fastq.gz\n",
            "Downloaded: ERR5885025_2.fastq.gz\n",
            "Downloaded: ERR5885028_1.fastq.gz\n",
            "Downloaded: ERR5885028_2.fastq.gz\n",
            "Downloaded: ERR5885029_1.fastq.gz\n",
            "Downloaded: ERR5885029_2.fastq.gz\n",
            "Downloaded: ERR5885030_1.fastq.gz\n",
            "Downloaded: ERR5885030_2.fastq.gz\n",
            "Downloaded: ERR5885032_1.fastq.gz\n",
            "Downloaded: ERR5885032_2.fastq.gz\n",
            "Downloaded: ERR5960499_1.fastq.gz\n",
            "Downloaded: ERR5960499_2.fastq.gz\n",
            "Downloaded: ERR6053339_1.fastq.gz\n",
            "Downloaded: ERR6053339_2.fastq.gz\n",
            "Downloaded: ERR6608942_1.fastq.gz\n",
            "Downloaded: ERR6608942_2.fastq.gz\n",
            "Downloaded: ERR6608944_1.fastq.gz\n",
            "Downloaded: ERR6608944_2.fastq.gz\n",
            "Downloaded: ERR6608943_1.fastq.gz\n",
            "Downloaded: ERR6608943_2.fastq.gz\n",
            "Downloaded: ERR5885026_1.fastq.gz\n",
            "Downloaded: ERR5885026_2.fastq.gz\n",
            "Downloaded: ERR5885027_1.fastq.gz\n",
            "Downloaded: ERR5885027_2.fastq.gz\n",
            "Downloaded: ERR5885031_1.fastq.gz\n",
            "Downloaded: ERR5885031_2.fastq.gz\n",
            "Downloaded: ERR5885033_1.fastq.gz\n",
            "Downloaded: ERR5885033_2.fastq.gz\n",
            "Downloaded: ERR6053337_1.fastq.gz\n",
            "Downloaded: ERR6053337_2.fastq.gz\n",
            "Downloaded: ERR6155194_1.fastq.gz\n",
            "Downloaded: ERR6155194_2.fastq.gz\n",
            "Downloaded: ERR6155195_1.fastq.gz\n",
            "Downloaded: ERR6155195_2.fastq.gz\n",
            "Downloaded: ERR6155196_1.fastq.gz\n",
            "Downloaded: ERR6155196_2.fastq.gz\n"
          ]
        }
      ],
      "source": [
        "MAX_FILE_SIZE = 500\n",
        "\n",
        "for ftp_link in ftp_links:\n",
        "  check_and_download_file(ftp_link, MAX_FILE_SIZE, DOWNLOAD_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decompress FASTQ.GZ files."
      ],
      "metadata": {
        "id": "Z19O7yMwCyCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DD805u49_prw"
      },
      "outputs": [],
      "source": [
        "def decompress_fastq_gz(input_file, output_file):\n",
        "    with gzip.open(input_file, 'rb') as f_in:\n",
        "        with open(output_file, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"/content/REPORT_FILES\"\n",
        "output_dir = \"/content/FASTQ_FILES\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through files in the input directory\n",
        "for file_name in os.listdir(input_dir):\n",
        "    if file_name.endswith('.fastq.gz'):\n",
        "        input_file_path = os.path.join(input_dir, file_name)\n",
        "        output_file_path = os.path.join(output_dir, file_name[:-3])  # Remove the .gz extension\n",
        "\n",
        "        # Decompress the file\n",
        "        decompress_fastq_gz(input_file_path, output_file_path)\n",
        "\n",
        "        print(f\"Decompressed: {file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjiGxTINCrv-",
        "outputId": "83d3d57d-1d26-4fa0-cff8-3cbc837d1077"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompressed: ERR6053337_1.fastq.gz\n",
            "Decompressed: ERR5960500_2.fastq.gz\n",
            "Decompressed: ERR5885028_2.fastq.gz\n",
            "Decompressed: ERR6155196_1.fastq.gz\n",
            "Decompressed: ERR5885029_2.fastq.gz\n",
            "Decompressed: ERR5960499_2.fastq.gz\n",
            "Decompressed: ERR6155196_2.fastq.gz\n",
            "Decompressed: ERR6053338_1.fastq.gz\n",
            "Decompressed: ERR6155194_1.fastq.gz\n",
            "Decompressed: ERR6155194_2.fastq.gz\n",
            "Decompressed: ERR5885030_1.fastq.gz\n",
            "Decompressed: ERR5960498_2.fastq.gz\n",
            "Decompressed: ERR5885030_2.fastq.gz\n",
            "Decompressed: ERR6608944_1.fastq.gz\n",
            "Decompressed: ERR6053339_1.fastq.gz\n",
            "Decompressed: ERR6608944_2.fastq.gz\n",
            "Decompressed: ERR5885033_2.fastq.gz\n",
            "Decompressed: ERR6608943_2.fastq.gz\n",
            "Decompressed: ERR6608942_2.fastq.gz\n",
            "Decompressed: ERR5885031_2.fastq.gz\n",
            "Decompressed: ERR5885023_1.fastq.gz\n",
            "Decompressed: ERR6155195_2.fastq.gz\n",
            "Decompressed: ERR6053339_2.fastq.gz\n",
            "Decompressed: ERR5960500_1.fastq.gz\n",
            "Decompressed: ERR5885033_1.fastq.gz\n",
            "Decompressed: ERR5885025_2.fastq.gz\n",
            "Decompressed: ERR5885027_2.fastq.gz\n",
            "Decompressed: ERR5960499_1.fastq.gz\n",
            "Decompressed: ERR5885032_1.fastq.gz\n",
            "Decompressed: ERR5885023_2.fastq.gz\n",
            "Decompressed: ERR6608942_1.fastq.gz\n",
            "Decompressed: ERR5885025_1.fastq.gz\n",
            "Decompressed: ERR5885026_2.fastq.gz\n",
            "Decompressed: ERR5885028_1.fastq.gz\n",
            "Decompressed: ERR5885031_1.fastq.gz\n",
            "Decompressed: ERR6053337_2.fastq.gz\n",
            "Decompressed: ERR5885026_1.fastq.gz\n",
            "Decompressed: ERR5885029_1.fastq.gz\n",
            "Decompressed: ERR5960498_1.fastq.gz\n",
            "Decompressed: ERR5885024_1.fastq.gz\n",
            "Decompressed: ERR6608943_1.fastq.gz\n",
            "Decompressed: ERR5885032_2.fastq.gz\n",
            "Decompressed: ERR5885024_2.fastq.gz\n",
            "Decompressed: ERR5885027_1.fastq.gz\n",
            "Decompressed: ERR6053338_2.fastq.gz\n",
            "Decompressed: ERR6155195_1.fastq.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the meta data of the FASTQ files."
      ],
      "metadata": {
        "id": "LxfaoU1yFsHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata_from_fastq(directory):\n",
        "    metadata = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".fastq\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, \"r\") as handle:\n",
        "                for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                    # Extract relevant metadata from each record\n",
        "                    record_metadata = {\n",
        "                        \"file_name\": filename,\n",
        "                        \"sequence_id\": record.id,\n",
        "                        \"sequence_length\": len(record.seq),\n",
        "                        \"quality_scores\": record.letter_annotations[\"phred_quality\"]\n",
        "                    }\n",
        "                    metadata.append(record_metadata)\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "5Tw6lTadFz6P"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing FASTQ files\n",
        "fastq_directory = '/content/FASTQ_FILES'\n",
        "\n",
        "# Extract metadata from FASTQ files\n",
        "fastq_metadata = extract_metadata_from_fastq(fastq_directory)\n",
        "\n",
        "# Example printing of metadata\n",
        "for record_metadata in fastq_metadata:\n",
        "    print(record_metadata)"
      ],
      "metadata": {
        "id": "_gtD96tkF9p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJIJhV-n_cL9"
      },
      "source": [
        "## Compress and save to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_BJ719-bex",
        "outputId": "af61bbe1-bb24-42f4-8f30-0a9da5fa4724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/fastq_files' has been zipped and uploaded to Google Drive with seed 2.\n"
          ]
        }
      ],
      "source": [
        "# Define the source folder and its contents\n",
        "source_folder = '/content/FASTQ_FILES'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(source_folder, 'zip', source_folder)\n",
        "\n",
        "# Move the zip file to Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "seed = str(SEED)\n",
        "new_filename = f'fastq_files_seed_{seed}_{timestamp}.zip'\n",
        "\n",
        "shutil.move(source_folder + '.zip', f'/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/{new_filename}')\n",
        "\n",
        "print(f\"Folder '{source_folder}' has been zipped and uploaded to Google Drive with seed {seed}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_gVeLJprX212",
        "KyX1AzRaHtAm",
        "dhnm-c1pH24p",
        "RuD4gOfxZXmS",
        "aMGlTzYlmNyf",
        "oBF1drz3D0dw"
      ],
      "provenance": [],
      "mount_file_id": "1lTVYho1DjXxlDX304Kd6xIDXjlq4s_lu",
      "authorship_tag": "ABX9TyOVwOZitoRsYhvxH4iol9f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}