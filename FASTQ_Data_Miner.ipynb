{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/FASTQ_Data_Miner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_oMPfTkAIs",
        "outputId": "7dc06715-9eaa-42c6-c868-2849ee0c780d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX1AzRaHtAm"
      },
      "source": [
        "## Import XML file with report file links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqdS032j2YwV"
      },
      "source": [
        "This XML file contains URLs for one or more report files. These report files contain the FASTQ files to be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRNTZp-zFgAC",
        "outputId": "45c3da94-4196-481d-dcfa-a6e9a17a5e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "!pip install rarfile\n",
        "import rarfile\n",
        "import zipfile\n",
        "\n",
        "def extract_files(source_path, destination_path):\n",
        "    try:\n",
        "        # Create the destination directory if it doesn't exist\n",
        "        os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "        # Check the file extension and extract or copy, accordingly\n",
        "        if source_path.endswith('.rar'):\n",
        "            with rarfile.RarFile(source_path) as rf:\n",
        "                rf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.zip'):\n",
        "            with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "                zf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.xml'):\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type. Only RAR, ZIP, and XML files are supported.\")\n",
        "\n",
        "        print(f\"Extraction completed successfully to: {destination_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2616fHGJi-",
        "outputId": "4b91b185-0a55-4912-b9c8-9ec3ed9e4893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed successfully to: /content/\n"
          ]
        }
      ],
      "source": [
        "# Define the source and destination paths\n",
        "dataset_path = \"/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/SOURCES/PRJEB44548.xml\"\n",
        "xml_path = \"/content/\"\n",
        "\n",
        "# Extract the files\n",
        "extract_files(dataset_path, xml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnm-c1pH24p"
      },
      "source": [
        "## Create a CSV file with the report file links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah-Xw3zARN9I",
        "outputId": "e15ad527-7624-4a2d-e63b-6b424dfcba72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Report file links saved to '/content/PRJEB44548_links.csv' successfully.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def extract_report_file_links(text):\n",
        "    report_files = []\n",
        "    lines = text.split('\\n')\n",
        "    index = 0\n",
        "\n",
        "    while index < len(lines):\n",
        "        line = lines[index]\n",
        "\n",
        "        if \"<DB>ENA-FASTQ-FILES</DB>\" in line:\n",
        "            # Search for the link below this line\n",
        "            next_line = lines[index + 1]\n",
        "\n",
        "            if \"<ID>\" in next_line:\n",
        "                link_start_index = next_line.find(\"<ID><![CDATA[\")\n",
        "\n",
        "                if link_start_index != -1:\n",
        "                    link_end_index = next_line.find(\"]]></ID>\")\n",
        "\n",
        "                    if link_end_index != -1:\n",
        "                        fastq_link = next_line[link_start_index+len(\"<ID><![CDATA[\"):link_end_index]\n",
        "                        report_files.append(fastq_link)\n",
        "\n",
        "            # Move to the next line after the link\n",
        "            index += 2\n",
        "        else:\n",
        "            index += 1\n",
        "\n",
        "    return report_files\n",
        "\n",
        "def read_xml_file(xml_path):\n",
        "    try:\n",
        "        with open(xml_path, 'r') as f:\n",
        "            xml_text = f.read()\n",
        "\n",
        "        return xml_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading XML file: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "def save_links_to_csv(links, xml_path):\n",
        "    # Extract CSV file name from XML file name\n",
        "    base_name = os.path.basename(xml_path)\n",
        "    csv_filename = os.path.splitext(base_name)[0] + \"_links.csv\"\n",
        "    csv_path = os.path.join(\"/content/\", csv_filename)\n",
        "\n",
        "    try:\n",
        "        with open(csv_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\"Report file Links\"])\n",
        "\n",
        "            for link in links:\n",
        "                writer.writerow([link])\n",
        "\n",
        "        print(f\"Report file links saved to '{csv_path}' successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV file: {e}\")\n",
        "\n",
        "# Read the XML file\n",
        "xml_path += os.path.basename(dataset_path)\n",
        "xml_text = read_xml_file(xml_path)\n",
        "\n",
        "if xml_text:\n",
        "    report_files = extract_report_file_links(xml_text)\n",
        "\n",
        "    if report_files:\n",
        "        save_links_to_csv(report_files, xml_path)\n",
        "\n",
        "    else:\n",
        "        print(\"No report file links found in the XML file.\")\n",
        "else:\n",
        "    print(\"Error reading XML file. Check the file path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUAhfeqaw3wq"
      },
      "source": [
        "Display the csv file information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjxSkFe2v9M3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "def display_csv_content(csv_path):\n",
        "    try:\n",
        "        # Create a Pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Display the DataFrame head and info\n",
        "        print(df.head())\n",
        "        print(df.info())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "\n",
        "display_csv_content(csv_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuD4gOfxZXmS"
      },
      "source": [
        "## Download the file reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQzbCZ0FK-Ex"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def extract_ftp_links_from_csv(csv_file):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Initialize a list to store the extracted FTP links\n",
        "    ftp_links = []\n",
        "\n",
        "def save_ftp_links_to_csv(ftp_links, output_csv):\n",
        "    # Create a DataFrame from the list of FTP links\n",
        "    df = pd.DataFrame({'FTP Links': ftp_links})\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Extracted FTP links saved to '{output_csv}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the input CSV file containing URLs and the output CSV file for FTP links\n",
        "    input_csv = '/content/PRJEB44548_links.csv'\n",
        "    output_csv = 'ftp_links_output.csv'\n",
        "\n",
        "    # Extract FTP links from the URLs in the CSV file\n",
        "    ftp_links = extract_ftp_links_from_csv(input_csv)\n",
        "\n",
        "    # Save the extracted FTP links to a new CSV file\n",
        "    save_ftp_links_to_csv(ftp_links, output_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7h6cH99ye-M"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SweHk9-egvz",
        "outputId": "897ef6ff-026f-44a9-cac8-24869ad54567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/003/ERR5885023/ERR5885023_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/003/ERR5885023/ERR5885023_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/005/ERR5885025/ERR5885025_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/005/ERR5885025/ERR5885025_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/008/ERR5885028/ERR5885028_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/008/ERR5885028/ERR5885028_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/009/ERR5885029/ERR5885029_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/009/ERR5885029/ERR5885029_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/000/ERR5885030/ERR5885030_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/000/ERR5885030/ERR5885030_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/002/ERR5885032/ERR5885032_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/002/ERR5885032/ERR5885032_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/009/ERR5960499/ERR5960499_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/009/ERR5960499/ERR5960499_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053339/ERR6053339_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053339/ERR6053339_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/004/ERR6608944/ERR6608944_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/004/ERR6608944/ERR6608944_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/004/ERR5885024/ERR5885024_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/004/ERR5885024/ERR5885024_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/008/ERR5960498/ERR5960498_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/008/ERR5960498/ERR5960498_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/000/ERR5960500/ERR5960500_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR596/000/ERR5960500/ERR5960500_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053338/ERR6053338_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053338/ERR6053338_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/006/ERR5885026/ERR5885026_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/006/ERR5885026/ERR5885026_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/007/ERR5885027/ERR5885027_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/007/ERR5885027/ERR5885027_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/001/ERR5885031/ERR5885031_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/001/ERR5885031/ERR5885031_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/003/ERR5885033/ERR5885033_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR588/003/ERR5885033/ERR5885033_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/007/ERR6053337/ERR6053337_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/007/ERR6053337/ERR6053337_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/004/ERR6155194/ERR6155194_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/004/ERR6155194/ERR6155194_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/005/ERR6155195/ERR6155195_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/005/ERR6155195/ERR6155195_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/006/ERR6155196/ERR6155196_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR615/006/ERR6155196/ERR6155196_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/003/ERR6608943/ERR6608943_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/003/ERR6608943/ERR6608943_2.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/002/ERR6608942/ERR6608942_1.fastq.gz\n",
            "ftp.sra.ebi.ac.uk/vol1/fastq/ERR660/002/ERR6608942/ERR6608942_2.fastq.gz\n",
            "Downloaded: ERR5885023_1.fastq.gz\n",
            "Downloaded: ERR5885023_2.fastq.gz\n",
            "Downloaded: ERR5885025_1.fastq.gz\n",
            "Downloaded: ERR5885025_2.fastq.gz\n",
            "Downloaded: ERR5885028_1.fastq.gz\n",
            "Downloaded: ERR5885028_2.fastq.gz\n",
            "Downloaded: ERR5885029_1.fastq.gz\n",
            "Downloaded: ERR5885029_2.fastq.gz\n",
            "Downloaded: ERR5885030_1.fastq.gz\n",
            "Downloaded: ERR5885030_2.fastq.gz\n",
            "Downloaded: ERR5885032_1.fastq.gz\n",
            "Downloaded: ERR5885032_2.fastq.gz\n",
            "Downloaded: ERR5960499_1.fastq.gz\n",
            "Downloaded: ERR5960499_2.fastq.gz\n",
            "Downloaded: ERR6053339_1.fastq.gz\n",
            "Downloaded: ERR6053339_2.fastq.gz\n",
            "Downloaded: ERR6608944_1.fastq.gz\n",
            "Downloaded: ERR6608944_2.fastq.gz\n",
            "Downloaded: ERR5885024_1.fastq.gz\n",
            "Downloaded: ERR5885024_2.fastq.gz\n",
            "Downloaded: ERR5960498_1.fastq.gz\n",
            "Downloaded: ERR5960498_2.fastq.gz\n",
            "Downloaded: ERR5960500_1.fastq.gz\n",
            "Downloaded: ERR5960500_2.fastq.gz\n",
            "Downloaded: ERR6053338_1.fastq.gz\n",
            "Downloaded: ERR6053338_2.fastq.gz\n",
            "Downloaded: ERR5885026_1.fastq.gz\n",
            "Downloaded: ERR5885026_2.fastq.gz\n",
            "Downloaded: ERR5885027_1.fastq.gz\n",
            "Downloaded: ERR5885027_2.fastq.gz\n",
            "Downloaded: ERR5885031_1.fastq.gz\n",
            "Downloaded: ERR5885031_2.fastq.gz\n",
            "Downloaded: ERR5885033_1.fastq.gz\n",
            "Downloaded: ERR5885033_2.fastq.gz\n",
            "Downloaded: ERR6053337_1.fastq.gz\n",
            "Downloaded: ERR6053337_2.fastq.gz\n",
            "Downloaded: ERR6155194_1.fastq.gz\n",
            "Downloaded: ERR6155194_2.fastq.gz\n",
            "Downloaded: ERR6155195_1.fastq.gz\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def download_reports(report_links, download_dir, num_files=None, seed=None):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        random.shuffle(report_links)\n",
        "\n",
        "    if num_files is not None:\n",
        "        report_links = report_links[:num_files]\n",
        "\n",
        "    for link in report_links:\n",
        "        file_name = link.split('/')[-1]\n",
        "        file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "        response = requests.get(link, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    f.write(chunk)\n",
        "\n",
        "df = pd.read_csv(\"/content/PRJEB44548_links.csv\")\n",
        "REPORT_LINKS = df['Report file Links']\n",
        "DOWNLOAD_DIR = \"/content/FILEREPORTS_FILES/\"\n",
        "NUM_FILES = 50\n",
        "SEED = 2\n",
        "\n",
        "download_reports(REPORT_LINKS, DOWNLOAD_DIR, NUM_FILES, SEED)\n",
        "\n",
        "file_path = \"/content/FILEREPORTS_FILES/filereport?accession=PRJEB44548&result=read_run&fields=run_accession,fastq_ftp,fastq_md5,fastq_bytes\"\n",
        "\n",
        "# Open the file\n",
        "with open(file_path, 'r') as file:\n",
        "    # Skip the first line (header)\n",
        "    next(file)\n",
        "\n",
        "    # Initialize a list to store FTP links\n",
        "    ftp_links = []\n",
        "\n",
        "    # Iterate through each line in the file\n",
        "    for line in file:\n",
        "        # Split the line based on tab ('\\t') delimiter\n",
        "        line_parts = line.strip().split('\\t')\n",
        "\n",
        "        # Get the FTP links from the second column\n",
        "        ftp_links.extend(line_parts[1].split(';'))\n",
        "\n",
        "# Print the extracted FTP links\n",
        "for ftp_link in ftp_links:\n",
        "    print(ftp_link)\n",
        "\n",
        "def check_and_download_file(ftp_link, max_file_size, download_dir):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= max_file_size:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")\n",
        "\n",
        "MAX_FILE_SIZE = 500\n",
        "\n",
        "for ftp_link in ftp_links:\n",
        "  check_and_download_file(ftp_link, MAX_FILE_SIZE, DOWNLOAD_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD805u49_prw",
        "outputId": "23addc60-6b0c-418f-8dbc-ec13f97d0482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decompressed: ERR5885029_2.fastq.gz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "input_dir = \"/content/FILEREPORTS_FILES\"\n",
        "output_dir = \"/content/FASTQ_FILES\"\n",
        "\n",
        "def decompress_fastq_gz(input_file, output_file):\n",
        "    with gzip.open(input_file, 'rb') as f_in:\n",
        "        with open(output_file, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through files in the input directory\n",
        "for file_name in os.listdir(input_dir):\n",
        "    if file_name.endswith('.fastq.gz'):\n",
        "        input_file_path = os.path.join(input_dir, file_name)\n",
        "        output_file_path = os.path.join(output_dir, file_name[:-3])  # Remove the .gz extension\n",
        "\n",
        "        # Decompress the file\n",
        "        decompress_fastq_gz(input_file_path, output_file_path)\n",
        "\n",
        "        print(f\"Decompressed: {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVU0Wv0cQ3Ym"
      },
      "outputs": [],
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "def clear_folder(folder_path):\n",
        "    files_in_folder = len(os.listdir(folder_path))\n",
        "    print(f\"{files_in_folder} files found.\")\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path,file)\n",
        "        os.remove(file_path)\n",
        "\n",
        "    print(f\"{files_in_folder} removed from {folder_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luAprXAobk_0",
        "outputId": "17c313b2-2f0e-45c5-b307-66d59ffaf82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46 files found.\n",
            "46 removed from /content/downloaded_files.\n"
          ]
        }
      ],
      "source": [
        "#clear_folder('/content/downloaded_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yqYAwWfSJz",
        "outputId": "721b89d9-fb4e-42a0-87fb-33dbbe350aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 1 FASTQ files to /content/report_files.\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "DOWNLOAD_DIR = \"/content/report_files\"\n",
        "NUM_OF_FILES_TO_DOWNLOAD = 50\n",
        "SEED = 2 # 12,10,6,8,14,42,38 (Notes for the previous downloads)\n",
        "\n",
        "# Call the function\n",
        "download_reports(fastq_links, DOWNLOAD_DIR, num_files=NUM_OF_FILES_TO_DOWNLOAD, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuLhpYq6g6tQ"
      },
      "source": [
        "## Download FASTQ files from FTP links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNmxEBJyzEX_"
      },
      "source": [
        "The files are compressed (.fastq.gz) and a maximum file size is defined before downloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UMfiHRplzC1"
      },
      "outputs": [],
      "source": [
        "# Define the maximum file size to download (MB)\n",
        "MAX_FILE_SIZE = 50\n",
        "\n",
        "# Specify the directory path to download files\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# Check file size before downloading and download a file from the ftp link\n",
        "def check_and_download_file(ftp_link):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= MAX_FILE_SIZE:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WwRTDwP77iy"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/fastq_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGrKKKafaTTW"
      },
      "outputs": [],
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "#clear_folder('/content/fastq_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZZvb97DzkE0",
        "outputId": "39675621-24d6-4b96-9039-25a3ce49fb5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: ERR5885024_1.fastq.gz\n",
            "Downloaded FASTQ files.\n"
          ]
        }
      ],
      "source": [
        "# Define the directory path containing the FTP links\n",
        "directory_path = \"/content/report_files/\"\n",
        "\n",
        "# Define the directory path in which the files will be downloaded\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# List all files in the directory\n",
        "all_files = os.listdir(directory_path)\n",
        "\n",
        "# Filter files that contain the necessary information\n",
        "info_files = [file for file in all_files if \"filereport\" in file]\n",
        "\n",
        "# Iterate through each info file\n",
        "for info_file in info_files:\n",
        "    file_path = os.path.join(directory_path, info_file)\n",
        "\n",
        "    # Read the content of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Take only the data line\n",
        "        lines = f.readlines()\n",
        "        data_line = lines[1]\n",
        "\n",
        "        #Check if there is data in the file and take only the first ftp link\n",
        "        if data_line is not None:\n",
        "            columns = data_line.strip().split('\\t')\n",
        "\n",
        "            if len(columns) >= 2:\n",
        "                ftp_link = columns[1]\n",
        "                ftp_link_1 = ftp_link.split(\";\")[0]\n",
        "                #print (ftp_link_1)\n",
        "\n",
        "                check_and_download_file(ftp_link_1) # Use ftp_link instead to download all files\n",
        "\n",
        "print(\"Downloaded FASTQ files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m72eyY3K81dM"
      },
      "source": [
        "## Decompress .FASTQ.GZ files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOmjB_M8yT8"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def decompress_gz_files(directory_path):\n",
        "    # List all files in the directory\n",
        "    all_files = os.listdir(directory_path)\n",
        "\n",
        "    # Filter files to keep only .gz files\n",
        "    gz_files = [file for file in all_files if file.endswith(\".gz\")]\n",
        "\n",
        "    # Decompress each .gz file\n",
        "    for gz_file in gz_files:\n",
        "        gz_file_path = os.path.join(directory_path, gz_file)\n",
        "\n",
        "        # Determine the output file name by removing the '.gz' extension\n",
        "        output_file_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "        # Decompress the .gz file\n",
        "        try:\n",
        "            with gzip.open(gz_file_path, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "            # Remove the original .gz file after decompression\n",
        "            os.remove(gz_file_path)\n",
        "\n",
        "        except (OSError, gzip.BadGzipFile) as e:\n",
        "            print(f\"Error decompressing {gz_file}: {e}\")\n",
        "\n",
        "    print(\"Decompression complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWI-SunrryG",
        "outputId": "510ff178-17be-49ee-a2fa-c41894fa9e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decompression complete.\n"
          ]
        }
      ],
      "source": [
        "# Specify the directory path containing .gz files\n",
        "directory_path = \"/content/fastq_files/\"\n",
        "\n",
        "# Call the function to decompress .gz files in the specified directory\n",
        "decompress_gz_files(directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJIJhV-n_cL9"
      },
      "source": [
        "## Save to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_BJ719-bex",
        "outputId": "af61bbe1-bb24-42f4-8f30-0a9da5fa4724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/fastq_files' has been zipped and uploaded to Google Drive with seed 2.\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Define the source folder and its contents\n",
        "source_folder = '/content/fastq_files'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(source_folder, 'zip', source_folder)\n",
        "\n",
        "# Move the zip file to Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "seed = str(SEED)\n",
        "new_filename = f'fastq_files_seed_{seed}_{timestamp}.zip'\n",
        "\n",
        "shutil.move(source_folder + '.zip', f'/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/{new_filename}')\n",
        "\n",
        "print(f\"Folder '{source_folder}' has been zipped and uploaded to Google Drive with seed {seed}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lTVYho1DjXxlDX304Kd6xIDXjlq4s_lu",
      "authorship_tag": "ABX9TyNYVuq0pghzwEV47F9uguqO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}