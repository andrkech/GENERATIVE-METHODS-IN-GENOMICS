{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/FASTQ_Data_Miner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gVeLJprX212"
      },
      "source": [
        "### Connect Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_oMPfTkAIs",
        "outputId": "abeb2294-bf91-4df0-a59d-462f395f853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53BLyNScX8AM"
      },
      "source": [
        "### Import Libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PCWR7WPX-4t",
        "outputId": "4676351e-c2c4-4f8b-aee2-3cff52edaa57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "!pip install rarfile\n",
        "import rarfile\n",
        "import zipfile\n",
        "import gzip\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import random\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX1AzRaHtAm"
      },
      "source": [
        "### Import XML file with report file links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqdS032j2YwV"
      },
      "source": [
        "The XML file contains links for one or more report files. These report files contain the FASTQ files to be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRNTZp-zFgAC"
      },
      "outputs": [],
      "source": [
        "def extract_files(source_path, destination_path):\n",
        "    try:\n",
        "        # Create the destination directory if it doesn't exist\n",
        "        os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "        # Check the file extension and extract or copy, accordingly\n",
        "        if source_path.endswith('.rar'):\n",
        "            with rarfile.RarFile(source_path) as rf:\n",
        "                rf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.zip'):\n",
        "            with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "                zf.extractall(destination_path)\n",
        "\n",
        "        elif source_path.endswith('.xml'):\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type. Only RAR, ZIP, and XML files are supported.\")\n",
        "\n",
        "        print(f\"Extraction completed successfully to: {destination_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2616fHGJi-",
        "outputId": "06f582c8-595e-4c83-88aa-d7b37e653118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully to: /content/\n"
          ]
        }
      ],
      "source": [
        "# Define the source and destination paths\n",
        "dataset_path = \"/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/SOURCES/PRJEB44548.xml\"\n",
        "xml_path = \"/content/\"\n",
        "\n",
        "# Extract the files\n",
        "extract_files(dataset_path, xml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnm-c1pH24p"
      },
      "source": [
        "### Create a CSV file with the report file links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ah-Xw3zARN9I"
      },
      "outputs": [],
      "source": [
        "def extract_report_file_links(text):\n",
        "    report_files = []\n",
        "    lines = text.split('\\n')\n",
        "    index = 0\n",
        "\n",
        "    while index < len(lines):\n",
        "        line = lines[index]\n",
        "\n",
        "        if \"<DB>ENA-FASTQ-FILES</DB>\" in line:\n",
        "            # Search for the link below this line\n",
        "            next_line = lines[index + 1]\n",
        "\n",
        "            if \"<ID>\" in next_line:\n",
        "                link_start_index = next_line.find(\"<ID><![CDATA[\")\n",
        "\n",
        "                if link_start_index != -1:\n",
        "                    link_end_index = next_line.find(\"]]></ID>\")\n",
        "\n",
        "                    if link_end_index != -1:\n",
        "                        fastq_link = next_line[link_start_index+len(\"<ID><![CDATA[\"):link_end_index]\n",
        "                        report_files.append(fastq_link)\n",
        "\n",
        "            # Move to the next line after the link\n",
        "            index += 2\n",
        "        else:\n",
        "            index += 1\n",
        "\n",
        "    return report_files\n",
        "\n",
        "def read_xml_file(xml_path):\n",
        "    try:\n",
        "        with open(xml_path, 'r') as f:\n",
        "            xml_text = f.read()\n",
        "\n",
        "        return xml_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading XML file: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "def save_links_to_csv(links, xml_path):\n",
        "    # Extract CSV file name from XML file name\n",
        "    base_name = os.path.basename(xml_path)\n",
        "    csv_filename = os.path.splitext(base_name)[0] + \"_links.csv\"\n",
        "    csv_path = os.path.join(\"/content/\", csv_filename)\n",
        "\n",
        "    try:\n",
        "        with open(csv_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\"Report file Links\"])\n",
        "\n",
        "            for link in links:\n",
        "                writer.writerow([link])\n",
        "\n",
        "        print(f\"Report file links saved to '{csv_path}' successfully.\")\n",
        "\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSjywkB4YMMC",
        "outputId": "10b0b2ff-1d32-457a-db93-e37c047227c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report file links saved to '/content/PRJEB44548_links.csv' successfully.\n"
          ]
        }
      ],
      "source": [
        "# Read the XML file\n",
        "xml_path += os.path.basename(dataset_path)\n",
        "xml_text = read_xml_file(xml_path)\n",
        "\n",
        "if xml_text:\n",
        "    report_files = extract_report_file_links(xml_text)\n",
        "\n",
        "    if report_files:\n",
        "        report_csv_path = save_links_to_csv(report_files, xml_path)\n",
        "\n",
        "    else:\n",
        "        print(\"No report file links found in the XML file.\")\n",
        "else:\n",
        "    print(\"Error reading XML file. Check the file path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUAhfeqaw3wq"
      },
      "source": [
        "Display the csv file information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CjxSkFe2v9M3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2e74d4ef-a31e-495f-ef3c-e42f2f9c6cd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef display_csv_content(csv_path):\\n    try:\\n        # Create a Pandas DataFrame\\n        df = pd.read_csv(csv_path)\\n\\n        # Display the DataFrame head and info\\n        print(df.head())\\n        print(df.info())\\n\\n    except Exception as e:\\n        print(f\"Error reading CSV file: {e}\")\\n\\ndisplay_csv_content(csv_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''\n",
        "def display_csv_content(csv_path):\n",
        "    try:\n",
        "        # Create a Pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Display the DataFrame head and info\n",
        "        print(df.head())\n",
        "        print(df.info())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "\n",
        "display_csv_content(csv_path)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuD4gOfxZXmS"
      },
      "source": [
        "### Download the report files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7h6cH99ye-M"
      },
      "source": [
        "Extract the desired number of file reports from the CSV file that contains the links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fcGcqEflgjNK"
      },
      "outputs": [],
      "source": [
        "def download_reports(report_links, download_dir, num_files=None, seed=None):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        random.shuffle(report_links)\n",
        "\n",
        "    if num_files is not None:\n",
        "        report_links = report_links[:num_files]\n",
        "\n",
        "    for link in report_links:\n",
        "        file_name = link.split('/')[-1]\n",
        "        file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "        response = requests.get(link, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    f.write(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-dbLngPmDXW"
      },
      "source": [
        "Define the hyperparameters and call the function to download the reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0MkeFWaEgl1B"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(report_csv_path)\n",
        "REPORT_LINKS = df['Report file Links']\n",
        "DOWNLOAD_DIR = \"/content/REPORT_FILES/\"\n",
        "NUM_FILES = 50\n",
        "SEED = 2\n",
        "\n",
        "download_reports(REPORT_LINKS, DOWNLOAD_DIR, NUM_FILES, SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMGlTzYlmNyf"
      },
      "source": [
        "### Extract the FTP links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SweHk9-egvz",
        "outputId": "732ed162-68e5-423d-bb16-3ff9055da53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46 FTPs saved.\n"
          ]
        }
      ],
      "source": [
        "for report_file in os.listdir(DOWNLOAD_DIR):\n",
        "    report_file_path = os.path.join(DOWNLOAD_DIR, report_file)\n",
        "\n",
        "    with open(report_file_path, 'r') as file:\n",
        "        # Skip the first line (header)\n",
        "        next(file)\n",
        "\n",
        "        # Initialize a list to store FTP links\n",
        "        ftp_links = []\n",
        "\n",
        "        # Iterate through each line in the file\n",
        "        for line in file:\n",
        "            # Split the line based on tab ('\\t') delimiter\n",
        "            line_parts = line.strip().split('\\t')\n",
        "\n",
        "            # Get the FTP links from the second column\n",
        "            ftp_links.extend(line_parts[1].split(';'))\n",
        "\n",
        "# Print the number of FTP links\n",
        "print(f\"{len(ftp_links)} FTPs saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6i0IvTWqkdv9"
      },
      "outputs": [],
      "source": [
        "def check_and_download_file(ftp_link, max_file_size, download_dir):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= max_file_size:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndmFsR4tFIe",
        "outputId": "90c42e73-b18e-4eb5-bee8-855236570835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: ERR5885024_1.fastq.gz\n",
            "Downloaded: ERR5885024_2.fastq.gz\n",
            "Downloaded: ERR5960498_1.fastq.gz\n",
            "Downloaded: ERR5960498_2.fastq.gz\n",
            "Downloaded: ERR5960500_1.fastq.gz\n",
            "Downloaded: ERR5960500_2.fastq.gz\n",
            "Downloaded: ERR6053338_1.fastq.gz\n",
            "Downloaded: ERR6053338_2.fastq.gz\n",
            "Downloaded: ERR5885023_1.fastq.gz\n",
            "Downloaded: ERR5885023_2.fastq.gz\n",
            "Downloaded: ERR5885025_1.fastq.gz\n",
            "Downloaded: ERR5885025_2.fastq.gz\n",
            "Downloaded: ERR5885028_1.fastq.gz\n",
            "Downloaded: ERR5885028_2.fastq.gz\n",
            "Downloaded: ERR5885029_1.fastq.gz\n",
            "Downloaded: ERR5885029_2.fastq.gz\n",
            "Downloaded: ERR5885030_1.fastq.gz\n",
            "Downloaded: ERR5885030_2.fastq.gz\n",
            "Downloaded: ERR5885032_1.fastq.gz\n",
            "Downloaded: ERR5885032_2.fastq.gz\n",
            "Downloaded: ERR5960499_1.fastq.gz\n",
            "Downloaded: ERR5960499_2.fastq.gz\n",
            "Downloaded: ERR6053339_1.fastq.gz\n",
            "Downloaded: ERR6053339_2.fastq.gz\n",
            "Downloaded: ERR6608942_1.fastq.gz\n",
            "Downloaded: ERR6608942_2.fastq.gz\n",
            "Downloaded: ERR6608944_1.fastq.gz\n",
            "Downloaded: ERR6608944_2.fastq.gz\n",
            "Downloaded: ERR6608943_1.fastq.gz\n",
            "Downloaded: ERR6608943_2.fastq.gz\n",
            "Downloaded: ERR5885026_1.fastq.gz\n",
            "Downloaded: ERR5885026_2.fastq.gz\n",
            "Downloaded: ERR5885027_1.fastq.gz\n",
            "Downloaded: ERR5885027_2.fastq.gz\n",
            "Downloaded: ERR5885031_1.fastq.gz\n",
            "Downloaded: ERR5885031_2.fastq.gz\n",
            "Downloaded: ERR5885033_1.fastq.gz\n",
            "Downloaded: ERR5885033_2.fastq.gz\n",
            "Downloaded: ERR6053337_1.fastq.gz\n",
            "Downloaded: ERR6053337_2.fastq.gz\n",
            "Downloaded: ERR6155194_1.fastq.gz\n",
            "Downloaded: ERR6155194_2.fastq.gz\n",
            "Downloaded: ERR6155195_1.fastq.gz\n",
            "Downloaded: ERR6155195_2.fastq.gz\n",
            "Downloaded: ERR6155196_1.fastq.gz\n",
            "Downloaded: ERR6155196_2.fastq.gz\n"
          ]
        }
      ],
      "source": [
        "MAX_FILE_SIZE = 500\n",
        "\n",
        "for ftp_link in ftp_links:\n",
        "  check_and_download_file(ftp_link, MAX_FILE_SIZE, DOWNLOAD_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD805u49_prw",
        "outputId": "23addc60-6b0c-418f-8dbc-ec13f97d0482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decompressed: ERR5885029_2.fastq.gz\n"
          ]
        }
      ],
      "source": [
        "input_dir = \"/content/FILEREPORTS_FILES\"\n",
        "output_dir = \"/content/FASTQ_FILES\"\n",
        "\n",
        "def decompress_fastq_gz(input_file, output_file):\n",
        "    with gzip.open(input_file, 'rb') as f_in:\n",
        "        with open(output_file, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through files in the input directory\n",
        "for file_name in os.listdir(input_dir):\n",
        "    if file_name.endswith('.fastq.gz'):\n",
        "        input_file_path = os.path.join(input_dir, file_name)\n",
        "        output_file_path = os.path.join(output_dir, file_name[:-3])  # Remove the .gz extension\n",
        "\n",
        "        # Decompress the file\n",
        "        decompress_fastq_gz(input_file_path, output_file_path)\n",
        "\n",
        "        print(f\"Decompressed: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DVU0Wv0cQ3Ym"
      },
      "outputs": [],
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "def clear_folder(folder_path):\n",
        "    files_in_folder = len(os.listdir(folder_path))\n",
        "    print(f\"{files_in_folder} files found.\")\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path,file)\n",
        "        os.remove(file_path)\n",
        "\n",
        "    print(f\"{files_in_folder} removed from {folder_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luAprXAobk_0",
        "outputId": "17c313b2-2f0e-45c5-b307-66d59ffaf82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46 files found.\n",
            "46 removed from /content/downloaded_files.\n"
          ]
        }
      ],
      "source": [
        "#clear_folder('/content/downloaded_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yqYAwWfSJz",
        "outputId": "721b89d9-fb4e-42a0-87fb-33dbbe350aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 1 FASTQ files to /content/report_files.\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "DOWNLOAD_DIR = \"/content/report_files\"\n",
        "NUM_OF_FILES_TO_DOWNLOAD = 50\n",
        "SEED = 2 # 12,10,6,8,14,42,38 (Notes for the previous downloads)\n",
        "\n",
        "# Call the function\n",
        "download_reports(fastq_links, DOWNLOAD_DIR, num_files=NUM_OF_FILES_TO_DOWNLOAD, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuLhpYq6g6tQ"
      },
      "source": [
        "## Download FASTQ files from FTP links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNmxEBJyzEX_"
      },
      "source": [
        "The files are compressed (.fastq.gz) and a maximum file size is defined before downloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UMfiHRplzC1"
      },
      "outputs": [],
      "source": [
        "# Define the maximum file size to download (MB)\n",
        "MAX_FILE_SIZE = 50\n",
        "\n",
        "# Specify the directory path to download files\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# Check file size before downloading and download a file from the ftp link\n",
        "def check_and_download_file(ftp_link):\n",
        "    if not ftp_link.startswith(\"http://\") and not ftp_link.startswith(\"https://\"):\n",
        "        ftp_link = \"https://\" + ftp_link\n",
        "\n",
        "    # Send a HEAD request to get the file metadata\n",
        "    response = requests.head(ftp_link)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content length from the response headers\n",
        "        content_length = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Convert content length to megabytes\n",
        "        content_length_mb = content_length / (1024 * 1024)\n",
        "\n",
        "        if content_length_mb <= MAX_FILE_SIZE:\n",
        "            # Download the file\n",
        "            file_name = ftp_link.split('/')[-1]\n",
        "            file_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Check if the file already exists\n",
        "            if not os.path.exists(file_path):\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    # Download the file\n",
        "                    file_response = requests.get(ftp_link, stream=True)\n",
        "                    for chunk in file_response.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                print(f\"Downloaded: {file_name}\")\n",
        "            else:\n",
        "                print(f\"File already exists: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping file: {ftp_link} (Size exceeds maximum file size)\")\n",
        "            '''\n",
        "            Based on this case, there could be an additional option to save these links\n",
        "            in order to use them for the creation of a different dataset that contains such large files:\n",
        "\n",
        "            skipped_files.append(ftp_link)\n",
        "\n",
        "            '''\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for file: {ftp_link}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WwRTDwP77iy"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/fastq_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGrKKKafaTTW"
      },
      "outputs": [],
      "source": [
        "# Clear the folder from previous downloads (optional)\n",
        "#clear_folder('/content/fastq_files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZZvb97DzkE0",
        "outputId": "39675621-24d6-4b96-9039-25a3ce49fb5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: ERR5885024_1.fastq.gz\n",
            "Downloaded FASTQ files.\n"
          ]
        }
      ],
      "source": [
        "# Define the directory path containing the FTP links\n",
        "directory_path = \"/content/report_files/\"\n",
        "\n",
        "# Define the directory path in which the files will be downloaded\n",
        "download_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# List all files in the directory\n",
        "all_files = os.listdir(directory_path)\n",
        "\n",
        "# Filter files that contain the necessary information\n",
        "info_files = [file for file in all_files if \"filereport\" in file]\n",
        "\n",
        "# Iterate through each info file\n",
        "for info_file in info_files:\n",
        "    file_path = os.path.join(directory_path, info_file)\n",
        "\n",
        "    # Read the content of the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Take only the data line\n",
        "        lines = f.readlines()\n",
        "        data_line = lines[1]\n",
        "\n",
        "        #Check if there is data in the file and take only the first ftp link\n",
        "        if data_line is not None:\n",
        "            columns = data_line.strip().split('\\t')\n",
        "\n",
        "            if len(columns) >= 2:\n",
        "                ftp_link = columns[1]\n",
        "                ftp_link_1 = ftp_link.split(\";\")[0]\n",
        "                #print (ftp_link_1)\n",
        "\n",
        "                check_and_download_file(ftp_link_1) # Use ftp_link instead to download all files\n",
        "\n",
        "print(\"Downloaded FASTQ files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m72eyY3K81dM"
      },
      "source": [
        "## Decompress .FASTQ.GZ files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOmjB_M8yT8"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def decompress_gz_files(directory_path):\n",
        "    # List all files in the directory\n",
        "    all_files = os.listdir(directory_path)\n",
        "\n",
        "    # Filter files to keep only .gz files\n",
        "    gz_files = [file for file in all_files if file.endswith(\".gz\")]\n",
        "\n",
        "    # Decompress each .gz file\n",
        "    for gz_file in gz_files:\n",
        "        gz_file_path = os.path.join(directory_path, gz_file)\n",
        "\n",
        "        # Determine the output file name by removing the '.gz' extension\n",
        "        output_file_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "        # Decompress the .gz file\n",
        "        try:\n",
        "            with gzip.open(gz_file_path, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "            # Remove the original .gz file after decompression\n",
        "            os.remove(gz_file_path)\n",
        "\n",
        "        except (OSError, gzip.BadGzipFile) as e:\n",
        "            print(f\"Error decompressing {gz_file}: {e}\")\n",
        "\n",
        "    print(\"Decompression complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oWI-SunrryG",
        "outputId": "510ff178-17be-49ee-a2fa-c41894fa9e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decompression complete.\n"
          ]
        }
      ],
      "source": [
        "# Specify the directory path containing .gz files\n",
        "directory_path = \"/content/fastq_files/\"\n",
        "\n",
        "# Call the function to decompress .gz files in the specified directory\n",
        "decompress_gz_files(directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJIJhV-n_cL9"
      },
      "source": [
        "## Save to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_BJ719-bex",
        "outputId": "af61bbe1-bb24-42f4-8f30-0a9da5fa4724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/fastq_files' has been zipped and uploaded to Google Drive with seed 2.\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Define the source folder and its contents\n",
        "source_folder = '/content/fastq_files'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(source_folder, 'zip', source_folder)\n",
        "\n",
        "# Move the zip file to Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "seed = str(SEED)\n",
        "new_filename = f'fastq_files_seed_{seed}_{timestamp}.zip'\n",
        "\n",
        "shutil.move(source_folder + '.zip', f'/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/{new_filename}')\n",
        "\n",
        "print(f\"Folder '{source_folder}' has been zipped and uploaded to Google Drive with seed {seed}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_gVeLJprX212",
        "53BLyNScX8AM",
        "KyX1AzRaHtAm",
        "dhnm-c1pH24p"
      ],
      "provenance": [],
      "mount_file_id": "1lTVYho1DjXxlDX304Kd6xIDXjlq4s_lu",
      "authorship_tag": "ABX9TyNj4FCl3V4/GILqSqXjz54S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}