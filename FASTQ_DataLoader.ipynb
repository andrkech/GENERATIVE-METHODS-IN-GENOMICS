{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b7OHL-Cv1dkL",
        "I0yGyfaTt6n3",
        "DKE1gaFlycvW",
        "MLiiBeYtyj73",
        "CdfH2RON1p9F",
        "PtfVuT_VgeLv",
        "_1iC3RB5gscc",
        "uj9rJ_kTiVdA",
        "bB0bKgk8ic8T",
        "-XCs01_gjKXX"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKfj9V/MKpqe6q68UoOrMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/FASTQ_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset."
      ],
      "metadata": {
        "id": "b7OHL-Cv1dkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect with Google Drive."
      ],
      "metadata": {
        "id": "I0yGyfaTt6n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PSDgVlCuuDbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6206b0-0a32-4d7e-dec0-58179eaf0aa0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries."
      ],
      "metadata": {
        "id": "DKE1gaFlycvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "!pip install -q Bio\n",
        "from Bio import SeqIO"
      ],
      "metadata": {
        "id": "O1oBS28VyDg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0014a58f-201d-4b5b-f4c4-22b7b77dabde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the dataset folder and import it into the Colab notebook."
      ],
      "metadata": {
        "id": "MLiiBeYtyj73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_or_create_zip(source_path, extract_path):\n",
        "    if os.path.isdir(source_path):\n",
        "        shutil.make_archive(extract_path, \"zip\", source_path)\n",
        "        print(f\"Created ZIP file from directory: {source_path}\")\n",
        "\n",
        "    elif os.path.isfile(source_path) and source_path.lower().endswith('.zip'):\n",
        "        with zipfile.ZipFile(source_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "            print(f\"Extraction completed successfully: {source_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: Invalid source path '{source_path}'. Must be a directory or a ZIP file.\")"
      ],
      "metadata": {
        "id": "Dfq3unrswb4F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define source and extraction paths\n",
        "source_path = \"/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/FASTQ_FILES.zip\"\n",
        "extract_path = \"/content/fastq_dataset\"\n",
        "\n",
        "# Extract or create ZIP file based on source path\n",
        "extract_or_create_zip(source_path, extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKr3_jFd6p_Z",
        "outputId": "030fb7a2-a761-4246-b246-20693814c2c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully: /content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/FASTQ_FILES.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the DataLoader."
      ],
      "metadata": {
        "id": "CdfH2RON1p9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a FASTQ Dataset Class. (not used)"
      ],
      "metadata": {
        "id": "PtfVuT_VgeLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xde4i1HfSIAg"
      },
      "outputs": [],
      "source": [
        "class FastqDataset:\n",
        "    def __init__(self, data_dir, batch_size, shuffle=True):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.file_list = self.get_file_list()\n",
        "        self.indexes = list(range(len(self.file_list)))\n",
        "        self.num_samples = self.calculate_num_samples()\n",
        "        self.num_batches = self.num_samples // self.batch_size\n",
        "\n",
        "    def calculate_num_samples(self):\n",
        "        total_samples = 0\n",
        "        for file_path in self.file_list:\n",
        "            for record in SeqIO.parse(file_path, 'fastq'):\n",
        "                total_samples += 1\n",
        "        return total_samples\n",
        "\n",
        "    def get_file_list(self):\n",
        "        return [os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir) if f.endswith(\".fastq\")]\n",
        "\n",
        "    def parse_fastq(self, file_path):\n",
        "        reads, qualities = [], []\n",
        "        for record in SeqIO.parse(file_path, 'fastq'):\n",
        "            reads.append(str(record.seq))\n",
        "            qualities.append(record.letter_annotations['phred_quality'])\n",
        "        return reads, qualities\n",
        "\n",
        "    def load_batch(self, batch_files):\n",
        "        batch_data = []\n",
        "        for file_path in batch_files:\n",
        "            reads, qualities = self.parse_fastq(file_path)\n",
        "            batch_data.extend(qualities)\n",
        "        return batch_data\n",
        "\n",
        "    def pad_batch(self, batch_data, batch_size):\n",
        "        padding_data = batch_data.copy()\n",
        "        while len(padding_data) < batch_size:\n",
        "            padding_data.append(batch_data[len(padding_data) % len(batch_data)])\n",
        "        return padding_data\n",
        "\n",
        "    def data_generator(self, num_samples=None):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.indexes)\n",
        "\n",
        "        total_samples = num_samples if num_samples is not None else len(self.indexes)\n",
        "        for start in range(0, total_samples, self.batch_size):\n",
        "            end = min(start + self.batch_size, total_samples)\n",
        "            batch_indexes = self.indexes[start:end]\n",
        "            batch_files = [self.file_list[i] for i in batch_indexes]\n",
        "            batch_data = self.load_batch(batch_files)\n",
        "\n",
        "            if len(batch_data) < self.batch_size:\n",
        "                batch_data = self.pad_batch(batch_data, self.batch_size)\n",
        "\n",
        "            # Convert to tensor and ensure the right shape\n",
        "            batch_data = tf.ragged.constant(batch_data, dtype=tf.int32).to_tensor(shape=(self.batch_size, -1, 1))\n",
        "            yield batch_data\n",
        "\n",
        "    def create_tf_dataset(self, num_samples=None):\n",
        "        output_signature = tf.TensorSpec(shape=(self.batch_size, None, 1), dtype=tf.int32)\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: self.data_generator(num_samples),\n",
        "            output_signature=output_signature\n",
        "        )\n",
        "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    def get_num_batches(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_generator()\n",
        "\n",
        "# Create the FASTQ dataset obect\n",
        "#fastq_dataset = FastqDataset(data_dir=DATA_DIR, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Quality Score Dataset Class."
      ],
      "metadata": {
        "id": "_1iC3RB5gscc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3og2hMkdecMJ"
      },
      "outputs": [],
      "source": [
        "class QualityScoreDataset:\n",
        "    def __init__(self, data_dir, buffer_size=10000, batch_size=32, shuffle=True, seed=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.file_list = self.get_file_list()\n",
        "\n",
        "    def get_file_list(self):\n",
        "        return [os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir) if f.endswith(\".fastq\")]\n",
        "\n",
        "    def parse_fastq(self, file_path):\n",
        "        qualities = []\n",
        "        for record in SeqIO.parse(file_path, 'fastq'):\n",
        "            qualities.append(record.letter_annotations['phred_quality'])\n",
        "        return qualities\n",
        "\n",
        "    def quality_scores_generator(self):\n",
        "        for file_path in self.file_list:\n",
        "            qualities = self.parse_fastq(file_path)\n",
        "            for quality in qualities:\n",
        "                if len(quality) == 300:\n",
        "                    yield tf.expand_dims(tf.convert_to_tensor(quality, dtype=tf.int32), axis=-1)\n",
        "\n",
        "    def create_tf_dataset(self):\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            self.quality_scores_generator,\n",
        "            output_signature=tf.TensorSpec(shape=(300, 1), dtype=tf.int32)\n",
        "        )\n",
        "        if self.shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=self.buffer_size, seed=self.seed)\n",
        "        dataset = dataset.batch(self.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    def calculate_sequence_insights(self, sample_size=None, seed=None):\n",
        "        insights = []\n",
        "        quality_scores = list(self.quality_scores_generator())\n",
        "\n",
        "        if sample_size:\n",
        "            if seed is not None:\n",
        "                random.seed(seed)\n",
        "\n",
        "            sampled_indices = random.sample(range(len(quality_scores)), min(sample_size, len(quality_scores)))\n",
        "            sampled_qualities = [quality_scores[i].numpy().flatten() for i in sampled_indices]\n",
        "\n",
        "            for seq_num, quality in zip(sampled_indices, sampled_qualities):\n",
        "                quality_array = np.array(quality)\n",
        "                mean_val = np.mean(quality_array)\n",
        "                std_val = np.std(quality_array)\n",
        "                min_val = np.min(quality_array)\n",
        "                max_val = np.max(quality_array)\n",
        "                insights.append({\n",
        "                    'Sequence Number': seq_num,\n",
        "                    'Mean': mean_val,\n",
        "                    'Standard Deviation': std_val,\n",
        "                    'Minimum': min_val,\n",
        "                    'Maximum': max_val\n",
        "                })\n",
        "        else:\n",
        "            for seq_num, quality in enumerate(quality_scores):\n",
        "                quality_array = quality.numpy().flatten()\n",
        "                mean_val = np.mean(quality_array)\n",
        "                std_val = np.std(quality_array)\n",
        "                min_val = np.min(quality_array)\n",
        "                max_val = np.max(quality_array)\n",
        "                insights.append({\n",
        "                    'Sequence Number': seq_num,\n",
        "                    'Mean': mean_val,\n",
        "                    'Standard Deviation': std_val,\n",
        "                    'Minimum': min_val,\n",
        "                    'Maximum': max_val\n",
        "                })\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def display_insights(self, insights):\n",
        "        print(f\"{'Sequence':<15} {'Mean':<15} {'Std Dev':<15} {'Min':<10} {'Max':<10}\")\n",
        "        print(\"=\"*65)\n",
        "        for insight in insights:\n",
        "            print(f\"{insight['Sequence Number']:<15} {insight['Mean']:<15.2f} {insight['Standard Deviation']:<15.2f} {insight['Minimum']:<10} {insight['Maximum']:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataset object."
      ],
      "metadata": {
        "id": "uj9rJ_kTiVdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define hyperparameters and create the object."
      ],
      "metadata": {
        "id": "qMEnnRTcitwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pO1ZDAhmecMK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "SEED = 42\n",
        "\n",
        "quality_dataset = QualityScoreDataset(data_dir=extract_path, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate and display the insights of the dataset."
      ],
      "metadata": {
        "id": "ENpAfDmNi011"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dI1sNN44GXFU"
      },
      "outputs": [],
      "source": [
        "#sequence_insights = quality_dataset.calculate_sequence_insights(sample_size=5, seed=42)\n",
        "#quality_dataset.display_insights(sequence_insights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Tensor dataset."
      ],
      "metadata": {
        "id": "bB0bKgk8ic8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = quality_dataset.create_tf_dataset()"
      ],
      "metadata": {
        "id": "2-y2poeuFU2V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Tensor dataset in Drive."
      ],
      "metadata": {
        "id": "-XCs01_gjKXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bACHVuu_EeTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "41ff4b22-7e88-4041-9974-6c049a130ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to /content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'DatasetV2' has no attribute 'save_spec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-30dca5dcfea0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mspec_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{save_name}_spec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset spec saved to {spec_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'DatasetV2' has no attribute 'save_spec'"
          ]
        }
      ],
      "source": [
        "# Directory to save the dataset\n",
        "save_dir = '/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET'\n",
        "save_name = f'phred_dataset_tf_{BATCH_SIZE}'\n",
        "full_save_path = os.path.join(save_dir, save_name)\n",
        "\n",
        "tf.data.Dataset.save(tf_dataset, full_save_path)\n",
        "print(f\"Dataset saved to {save_dir}\")\n",
        "\n",
        "spec_path = os.path.join(save_dir, f'{save_name}_spec')\n",
        "tf.data.Dataset.save_spec(tf_dataset, spec_path)\n",
        "print(f\"Dataset spec saved to {spec_path}\")"
      ]
    }
  ]
}