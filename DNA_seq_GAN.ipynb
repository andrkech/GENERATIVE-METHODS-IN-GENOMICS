{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/DNA_seq_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01blEsAF3jw"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc4vWOvgGF-T",
        "outputId": "96fb12ea-c08c-47b4-ee65-8d9093f97348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "!pip install -q biopython\n",
        "from Bio import SeqIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fniVQSsyw4Sf",
        "outputId": "a29575e3-6047-4c1a-9698-d5b9f94ba7e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rXfyGpMAGEg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c71235-3d7b-42f1-dcc3-825079fab9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GHkUbiOgnXdM"
      },
      "outputs": [],
      "source": [
        "# Path to your ZIP file\n",
        "zip_file_path = '/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/filtered_reads.zip'\n",
        "\n",
        "# Directory to extract the ZIP file contents\n",
        "extract_dir = '/content/filtered_reads'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ggNp0pDtDFW"
      },
      "outputs": [],
      "source": [
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Now you can access the extracted data\n",
        "data_dir = os.path.join(extract_dir, 'filtered_reads')\n",
        "\n",
        "# Define data directory containing FASTQ files\n",
        "data_dir = '/content/filtered_reads'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbCRFrlnGYwy"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FSjPnKG_ZFZP"
      },
      "outputs": [],
      "source": [
        "class FastqDataset(Sequence):\n",
        "    def __init__(self, data_dir, batch_size=4, shuffle=True):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.file_list = self.get_file_list()\n",
        "        self.indexes = list(range(len(self.file_list)))\n",
        "        if self.file_list:\n",
        "            random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_files = [self.file_list[i] for i in batch_indexes]\n",
        "        batch_data = self.load_batch(batch_files)\n",
        "\n",
        "        # Filter out batches with 0 size\n",
        "        batch_data = [data for data in batch_data if data]\n",
        "\n",
        "        return batch_data\n",
        "\n",
        "    def get_file_list(self):\n",
        "        file_list = []\n",
        "        for filename in os.listdir(self.data_dir):\n",
        "            if filename.endswith(\".fastq\"):\n",
        "                file_path = os.path.join(self.data_dir, filename)\n",
        "                file_list.append(file_path)\n",
        "\n",
        "        return file_list\n",
        "\n",
        "    def load_batch(self, batch_files):\n",
        "        batch_data = []\n",
        "        for file_path in batch_files:\n",
        "            reads, qualities = self.parse_fastq(file_path)\n",
        "            batch_data.extend(zip(reads, qualities))\n",
        "        return batch_data\n",
        "\n",
        "    def parse_fastq(self, file_path):\n",
        "        reads, qualities = [], []\n",
        "\n",
        "        for record in SeqIO.parse(file_path, 'fastq'):\n",
        "            reads.append(str(record.seq))\n",
        "            qualities.append(record.letter_annotations['phred_quality'])\n",
        "\n",
        "        return reads, qualities\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom dataset\n",
        "dataset = FastqDataset(data_dir)\n",
        "\n",
        "# Example usage of the dataset\n",
        "for batch_data in dataset:\n",
        "    # Process batch_data as needed\n",
        "    print(\"Batch Size:\", len(batch_data))"
      ],
      "metadata": {
        "id": "jm6Gy1xmj3wK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981a7198-b006-498e-833e-be605306adcd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 189675\n",
            "Batch Size: 153468\n",
            "Batch Size: 149071\n",
            "Batch Size: 303648\n",
            "Batch Size: 78490\n",
            "Batch Size: 281819\n",
            "Batch Size: 18271\n",
            "Batch Size: 174967\n",
            "Batch Size: 277538\n",
            "Batch Size: 466566\n",
            "Batch Size: 172910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw14qBJ0Fdkc"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aS3zIeTp0JhQ"
      },
      "outputs": [],
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, latent_dim, seq_length):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.fc1 = layers.Dense(128, activation='relu')\n",
        "        self.fc2 = layers.Dense(256, activation='relu')\n",
        "        self.fc3 = layers.Dense(seq_length * 4, activation='relu')\n",
        "        self.fc4 = layers.Dense (seq_length)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNf5ofzfFk_o"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1lUvsPUlFn6l"
      },
      "outputs": [],
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self, sequence_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = layers.Dense(256, activation='relu')\n",
        "        self.fc2 = layers.Dense(128, activation='relu')\n",
        "        self.fc3 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkl2DoIjFqdD"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "LATENT_DIM = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "SEQ_LENGTH = 300"
      ],
      "metadata": {
        "id": "qAynwxhijKBI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3zJteYUP0Nd6"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, generator, discriminator, latent_dim=LATENT_DIM, lr=LEARNING_RATE):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.lr = lr\n",
        "\n",
        "        self.gen_optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        self.disc_optimizer = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "    def compile(self):\n",
        "        self.generator.compile(optimizer=self.gen_optimizer)\n",
        "        self.discriminator.compile(optimizer=self.disc_optimizer)\n",
        "\n",
        "    def train_step(self, real_data):\n",
        "        batch_size = tf.shape(real_data)[0]\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            fake_data = self.generator(noise, training=True)\n",
        "\n",
        "            real_pred = self.discriminator(real_data, training=True)\n",
        "            fake_pred = self.discriminator(fake_data, training=True)\n",
        "\n",
        "            gen_loss = tf.keras.losses.MeanSquaredError()(real_data, fake_data)\n",
        "            disc_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_pred), real_pred) + \\\n",
        "                        tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_pred), fake_pred)\n",
        "\n",
        "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "        return {\"Generator Loss\": gen_loss, \"Discriminator Loss\": disc_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6GPA50UGyMr"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CIUTctQpsfVE"
      },
      "outputs": [],
      "source": [
        "# Instantiate Generator and Discriminator\n",
        "generator = Generator(latent_dim=100, seq_length=SEQ_LENGTH)\n",
        "discriminator = Discriminator(sequence_length=SEQ_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate GAN model\n",
        "gan_model = GAN(generator, discriminator)\n",
        "\n",
        "gan_model.compile()"
      ],
      "metadata": {
        "id": "ViUfA9M6t3xi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "LMPW_loUuMLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary mapping nucleotides to integers\n",
        "nucleotide_to_index = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "def encode_sequence(sequence):\n",
        "    encoded_sequence = np.zeros((len(sequence), 4))\n",
        "    for i, nucleotide in enumerate(sequence):\n",
        "        if nucleotide in nucleotide_to_index:\n",
        "            index = nucleotide_to_index[nucleotide]\n",
        "            encoded_sequence[i, index] = 1\n",
        "    return encoded_sequence\n",
        "\n",
        "# Example usage\n",
        "sequence = \"GCGCCGATCTAAAGTCATTTGACTTAGGCGACGAGCTTGTCACTGATCCTTATGAAGATTTTCAAGAAAACTGGAACACTAAACATAGCAGTGGTGTTACCCGTGAACTCATGCGTGAGCTTAACGGAGGGGCATACACTCGCTATGTCGATAACAACTTCTGTGGCCCTGATGGCTACCCTCTTGAGTGCATTAAAGACCTTCTAGCACGTGCTGGTAAAGCTTCATGCACTTTGTCTGAACAACTGGACTTTNTTGACACTAAGAGGGGTGTATACTGCTGCCGTGAACATGAGCCTG\"\n",
        "encoded_sequence = encode_sequence(sequence)\n",
        "print(encoded_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7i6IK2gHnY6",
        "outputId": "18a93433-95b0-497c-c583-355a87784362"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 4\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_data in dataset:\n",
        "        # Filter out batches with zero size\n",
        "        if len(batch_data) == 0:\n",
        "            continue\n",
        "\n",
        "        # Convert each element of the batch to a TensorFlow tensor\n",
        "        batch_tensors = []\n",
        "        for data in batch_data:\n",
        "            reads_tensor = tf.convert_to_tensor(encode_sequence(data[0]), dtype=tf.float32)\n",
        "            qualities_tensor = tf.convert_to_tensor(data[1], dtype=tf.float32)\n",
        "            batch_tensors.append((reads_tensor, qualities_tensor))\n",
        "\n",
        "        # Perform forward pass\n",
        "        with tf.device('/device:GPU:0'):\n",
        "            loss_dict = gan_model.train_step(batch_tensors)\n",
        "\n",
        "        # Perform backward pass and update weights\n",
        "        gen_gradients = loss_dict['Generator Loss']\n",
        "        disc_gradients = loss_dict['Discriminator Loss']\n",
        "\n",
        "        optimizer.apply_gradients(zip(gen_gradients, gan_model.generator.trainable_variables))\n",
        "        optimizer.apply_gradients(zip(disc_gradients, gan_model.discriminator.trainable_variables))\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Generator Loss: {loss_dict[\"Generator Loss\"]}, Discriminator Loss: {loss_dict[\"Discriminator Loss\"]}')\n"
      ],
      "metadata": {
        "id": "AXq9nzJvuLga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "1aab11dc-40a6-4d59-eb28-f88a24c0306d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [300,4] != values[1].shape = [300] [Op:Pack] name: 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8356b7a79deb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Perform backward pass and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-1c8da7de4de0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, real_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [300,4] != values[1].shape = [300] [Op:Pack] name: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Generation"
      ],
      "metadata": {
        "id": "0oV3yO8AvPJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 5\n",
        "noise = tf.random.normal([NUM_SAMPLES, 100])\n",
        "generated_samples = generator(noise)"
      ],
      "metadata": {
        "id": "9bzaoqynvUGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in generated_samples:\n",
        "    print(sample)"
      ],
      "metadata": {
        "id": "NCu2Rttxv2ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEKvuRh+zBFU4+XzQC7ic3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}