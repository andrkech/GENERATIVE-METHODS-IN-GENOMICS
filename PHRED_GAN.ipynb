{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrkech/GENERATIVE-METHODS-IN-GENOMICS/blob/main/PHRED_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01blEsAF3jw"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc4vWOvgGF-T",
        "outputId": "f034570e-e0ad-448c-a2eb-de03adeb6e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "!pip install -q biopython\n",
        "from Bio import SeqIO\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQWu76uMkRcl"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idY4ueitn_lT",
        "outputId": "001da5be-0046-4ee1-9a1e-27cefb3bc899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(gpu)\n",
        "    tf.config.experimental.set_memory_growth(gpu,True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P90wkR1TkUyB"
      },
      "source": [
        "### Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXfyGpMAGEg1",
        "outputId": "d4b1d693-edeb-4090-8611-f213a0b301bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHkUbiOgnXdM"
      },
      "outputs": [],
      "source": [
        "# Path to your ZIP file\n",
        "zip_file_path = '/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/filtered_reads.zip'\n",
        "\n",
        "# Directory to extract the ZIP file contents\n",
        "extract_dir = '/content/filtered_reads'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ggNp0pDtDFW"
      },
      "outputs": [],
      "source": [
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxm6ocARqwIz"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "IU-dIRN0qvU2",
        "outputId": "9bb8d08c-2255-4d14-d001-e24cbe15442b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nCALLBACKS = [\\n    tf.keras.callbacks.ModelCheckpoint(filepath='gan_checkpoint.h5', save_best_only=True),\\n    tf.keras.callbacks.EarlyStopping(patience=3, monitor='discriminator_loss', restore_best_weights=True),\\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir)  # TensorBoard callback\\n]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "LATENT_DIM = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "SEQ_LENGTH = 300\n",
        "NUM_EPOCHS = 20\n",
        "PATIENCE = 5\n",
        "NUM_SAMPLES = 10 # to be generated from the GAN\n",
        "\n",
        "LOG_DIR = './logs/gan'\n",
        "log_dir = os.path.join(LOG_DIR, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "'''\n",
        "CALLBACKS = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='gan_checkpoint.h5', save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, monitor='discriminator_loss', restore_best_weights=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=log_dir)  # TensorBoard callback\n",
        "]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PJg4jHWY2rJ"
      },
      "source": [
        "### Load Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74be9jY8H1Yw",
        "outputId": "874aecd7-ae70-4e80-ed33-95d3ac1b585f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "tf_dataset_dir = '/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET'\n",
        "tf_dataset = tf.data.Dataset.load(tf_dataset_dir)\n",
        "print(\"Dataset loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLj9RnEW96nP",
        "outputId": "c5b9c8ac-6d1a-46c8-fe30-323dc6bff94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[34.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [36.]\n",
            "  [18.]\n",
            "  [20.]]\n",
            "\n",
            " [[31.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [27.]\n",
            "  [26.]\n",
            "  [27.]]\n",
            "\n",
            " [[34.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [37.]\n",
            "  [37.]\n",
            "  [37.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[34.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [34.]\n",
            "  [34.]\n",
            "  [24.]]\n",
            "\n",
            " [[34.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [36.]\n",
            "  [37.]\n",
            "  [37.]]\n",
            "\n",
            " [[34.]\n",
            "  [34.]\n",
            "  [34.]\n",
            "  ...\n",
            "  [ 7.]\n",
            "  [ 7.]\n",
            "  [ 7.]]], shape=(32, 300, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(tf_dataset)\n",
        "first_element = next(iterator)\n",
        "print(first_element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mFqF0e0S9AXX"
      },
      "outputs": [],
      "source": [
        "def plot_quality_distributions(tf_dataset):\n",
        "    for batch, qualities_batch in enumerate(tf_dataset):\n",
        "        for i in range(qualities_batch.shape[0]):\n",
        "            plt.hist(qualities_batch[i,:,0], bins=20)\n",
        "            plt.title(f\"Quality Score Distribution for Sequence {i} in Batch {batch}\")\n",
        "            plt.xlabel(\"Quality Score\")\n",
        "            plt.ylabel(\"Frequency\")\n",
        "            plt.show()\n",
        "\n",
        "#plot_quality_distributions(tf_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw14qBJ0Fdkc"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS3zIeTp0JhQ"
      },
      "outputs": [],
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, seq_length, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.dense1 = tf.keras.layers.Dense(256, input_shape=(latent_dim,))\n",
        "        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n",
        "        self.dense2 = tf.keras.layers.Dense(512)\n",
        "        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n",
        "        self.dense3 = tf.keras.layers.Dense(seq_length)\n",
        "        self.reshape = tf.keras.layers.Reshape((seq_length, 1))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.leaky_relu1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.leaky_relu2(x)\n",
        "        x = self.dense3(x)\n",
        "        return self.reshape(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNf5ofzfFk_o"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lUvsPUlFn6l"
      },
      "outputs": [],
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self, seq_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.flatten = tf.keras.layers.Flatten(input_shape=(seq_length, 1))\n",
        "        self.dense1 = tf.keras.layers.Dense(512)\n",
        "        self.leaky_relu1 = tf.keras.layers.LeakyReLU()\n",
        "        self.dense2 = tf.keras.layers.Dense(256)\n",
        "        self.leaky_relu2 = tf.keras.layers.LeakyReLU()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.leaky_relu1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.leaky_relu2(x)\n",
        "        return self.dense3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkl2DoIjFqdD"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zJteYUP0Nd6"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, generator, discriminator, latent_dim=LATENT_DIM, lr=LEARNING_RATE):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.lr = lr\n",
        "\n",
        "        self.gen_optimizer = tf.keras.optimizers.Adam(2*lr) # Train generator at a higher rate\n",
        "        self.disc_optimizer = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "    def compile(self, disc_optimizer, gen_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.disc_optimizer = disc_optimizer\n",
        "        self.gen_optimizer = gen_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_data):\n",
        "        batch_size = tf.shape(real_data)[0]\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            fake_data = self.generator(noise, training=True)\n",
        "\n",
        "            real_pred = self.discriminator(real_data, training=True)\n",
        "            fake_pred = self.discriminator(fake_data, training=True)\n",
        "\n",
        "            gen_loss = tf.keras.losses.MeanSquaredError()(real_data, fake_data)\n",
        "            disc_loss_real = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_pred), real_pred)\n",
        "            disc_loss_fake = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_pred), fake_pred)\n",
        "            disc_loss = disc_loss_real + disc_loss_fake\n",
        "\n",
        "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "        return {\n",
        "            \"Generator Loss\": gen_loss,\n",
        "            \"Discriminator Loss\": disc_loss\n",
        "        }\n",
        "\n",
        "    def fit(self, dataset, num_epochs, log_dir, patience=5):\n",
        "        # Create a summary writer for TensorBoard\n",
        "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "        checkpoint_dir = './checkpoints'\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Early stopping variables\n",
        "        best_disc_loss = float('inf')\n",
        "        epochs_without_improvement = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "            # Initialize progress bar\n",
        "            epoch_progress = tqdm(total=len(dataset), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "            for batch, qualities_batch in enumerate(dataset):\n",
        "                # Perform forward pass\n",
        "                loss_dict = self.train_step(qualities_batch)\n",
        "\n",
        "                if loss_dict is None:\n",
        "                    print(\"train_step did not return any values. Please check the train_step method.\")\n",
        "                    return\n",
        "\n",
        "                # Update progress bar with loss values\n",
        "                epoch_progress.set_postfix(\n",
        "                    gen_loss=float(loss_dict[\"Generator Loss\"]),\n",
        "                    disc_loss=float(loss_dict[\"Discriminator Loss\"])\n",
        "                )\n",
        "                epoch_progress.update(1)\n",
        "\n",
        "                # Log the losses to TensorBoard\n",
        "                with summary_writer.as_default():\n",
        "                    tf.summary.scalar('Generator Loss', loss_dict[\"Generator Loss\"], step=epoch * len(dataset) + batch)\n",
        "                    tf.summary.scalar('Discriminator Loss', loss_dict[\"Discriminator Loss\"], step=epoch * len(dataset) + batch)\n",
        "\n",
        "            epoch_progress.close()\n",
        "\n",
        "            # Print the losses after each epoch\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs} completed.')\n",
        "            print(f'Generator Loss: {loss_dict[\"Generator Loss\"]}, Discriminator Loss: {loss_dict[\"Discriminator Loss\"]}')\n",
        "\n",
        "            # Save model checkpoints manually\n",
        "            self.generator.save_weights(os.path.join(checkpoint_dir, f'generator_epoch_{epoch+1}.ckpt'))\n",
        "            self.discriminator.save_weights(os.path.join(checkpoint_dir, f'discriminator_epoch_{epoch+1}.ckpt'))\n",
        "\n",
        "            # Early stopping check\n",
        "            if loss_dict[\"Discriminator Loss\"] < best_disc_loss:\n",
        "                best_disc_loss = loss_dict[\"Discriminator Loss\"]\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
        "                    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6GPA50UGyMr"
      },
      "source": [
        "### Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVE-cl13zHee"
      },
      "outputs": [],
      "source": [
        "# defining optimizer for Models\n",
        "gen_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "# Define Loss Function\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIUTctQpsfVE"
      },
      "outputs": [],
      "source": [
        "# Instantiate Generator and Discriminator\n",
        "generator = Generator(latent_dim=LATENT_DIM, seq_length=SEQ_LENGTH)\n",
        "discriminator = Discriminator(seq_length=SEQ_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUfA9M6t3xi"
      },
      "outputs": [],
      "source": [
        "# Instanciate GAN model\n",
        "gan_model = GAN(generator, discriminator, LATENT_DIM, LEARNING_RATE)\n",
        "\n",
        "gan_model.compile(\n",
        "    disc_optimizer=disc_optimizer,\n",
        "    gen_optimizer=gen_optimizer,\n",
        "    loss_fn=cross_entropy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMPW_loUuMLI"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "collapsed": true,
        "id": "wIuGY_duq3xJ",
        "outputId": "6bd15f4d-5df0-471a-df74-c39c09b47916"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 75081/75081 [13:21<00:00, 93.63batch/s, disc_loss=1.61e-5, gen_loss=5.8] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 completed.\n",
            "Generator Loss: 5.802612781524658, Discriminator Loss: 1.6096091712825e-05\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 75081/75081 [13:21<00:00, 93.63batch/s, disc_loss=0.000107, gen_loss=5.97] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 completed.\n",
            "Generator Loss: 5.965404510498047, Discriminator Loss: 0.0001069755235221237\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 75081/75081 [12:21<00:00, 101.20batch/s, disc_loss=1.45e-5, gen_loss=5.58]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 completed.\n",
            "Generator Loss: 5.580908298492432, Discriminator Loss: 1.4499837561743334e-05\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 75081/75081 [12:39<00:00, 98.80batch/s, disc_loss=2.92e-5, gen_loss=4.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 completed.\n",
            "Generator Loss: 4.237154006958008, Discriminator Loss: 2.9229006031528115e-05\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 75081/75081 [14:21<00:00, 87.11batch/s, disc_loss=0.000121, gen_loss=4.47] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 completed.\n",
            "Generator Loss: 4.471123218536377, Discriminator Loss: 0.00012096053978893906\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20:   1%|          | 718/75081 [00:07<12:21, 100.23batch/s, disc_loss=4.52e-6, gen_loss=66]  "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ff96785b3f45>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d6034b515a07>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, num_epochs, log_dir, patience)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# Update progress bar with loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 epoch_progress.set_postfix(\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mgen_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Generator Loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mdisc_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Discriminator Loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "training_history = gan_model.fit(tf_dataset, NUM_EPOCHS, LOG_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY16gg0u7hLB"
      },
      "source": [
        "### Visualize training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRpn80s37fjk"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs/gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oV3yO8AvPJa"
      },
      "source": [
        "### Sample Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bzaoqynvUGa"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal([NUM_SAMPLES, 100])\n",
        "generated_samples = generator(noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCu2Rttxv2ta"
      },
      "outputs": [],
      "source": [
        "print(generated_samples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0fvuRNvZ6cg"
      },
      "outputs": [],
      "source": [
        "generated_samples_np = generated_samples.numpy().reshape(NUM_SAMPLES, SEQ_LENGTH)\n",
        "\n",
        "save_samples_dir = '/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET'\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "filename = f\"generated_samples_{timestamp}.npy\"\n",
        "\n",
        "np.save(save_samples_dir, generated_samples_np)\n",
        "print(f\"Generated samples saved to {save_samples_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "mw14qBJ0Fdkc",
        "NNf5ofzfFk_o"
      ],
      "authorship_tag": "ABX9TyNIXuwJCNqbMBVFJqUvZ17W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}